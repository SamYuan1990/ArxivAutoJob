arxiv_id,date,title,position_index,initiative_index,position_index_comments,,initiative_index_comments,downloaded
2201.11903,2022-01-28,Chain-of-Thought Prompting Elicits Reasoning \\ in Large Language Models,prompt,hard code style,"The paper focuses on chain-of-thought prompting, which is a prompting technique that enhances reasoning in large language models without requiring training or fine-tuning. It falls under the 'prompt' category as it involves improving single-agent reasoning through prompt engineering rather than multi-agent systems, external data management, or model training.",,"Chain-of-thought prompting requires manually crafting prompts with exemplars that include intermediate reasoning steps. This is a hard-coded approach where humans design the prompting structure and provide specific reasoning demonstrations, rather than automated adjustment mechanisms or code implementations.",TRUE
2205.11916,2022-05-24,Formatting Instructions For NeurIPS 2022,prompt,hard code style,"The paper focuses on prompt engineering techniques for eliciting reasoning capabilities from pretrained large language models. It introduces Zero-shot-CoT, which uses a simple prompt template 'Let's think step by step' to facilitate step-by-step reasoning without requiring task-specific examples or fine-tuning.",,"The approach uses a fixed, manually designed prompt template ('Let's think step by step') that is applied uniformly across all tasks without modification. The prompting strategy is hard-coded and not automatically adjusted based on metrics or left to the LLM to determine.",TRUE
2305.10601,2023-05-17,Deliberate Search in Tree of Thoughts,prompt , auto adjust leave to LLM ,"The paper introduces Tree of Thoughts (ToT), a framework that enhances language model inference by enabling exploration over coherent units of text ('thoughts') as intermediate steps toward problem solving. This involves considering multiple reasoning paths, self-evaluating choices, and using search algorithms like BFS or DFS. The framework operates during inference and involves prompting the LM to generate and evaluate thoughts, as well as managing the exploration of different reasoning paths, which aligns with prompt and external data management aspects.",,"The framework uses language models to self-evaluate the progress of intermediate thoughts and generate diverse reasoning paths through prompting. The search algorithms (e.g., BFS, DFS) are hard-coded to systematically explore the tree of thoughts, but the generation and evaluation of thoughts are left to the LLM, indicating a combination of hard-coded and LLM-driven adjustments.",TRUE
2210.03629,2022-10-06,: Synergizing Reasoning and Acting in Language Models,prompt,auto adjust leave to LLM,"This paper focuses on synergizing reasoning and acting in language models, which involves integrating reasoning processes with action execution. This aligns with prompt engineering approaches where specific prompting techniques (like ReAct) are used to guide LLMs to alternate between reasoning traces and task-specific actions.",,"The approach described involves designing prompting strategies that enable language models to dynamically alternate between reasoning and acting. This represents an auto-adjust mechanism where the LLM itself determines when to reason and when to act based on the context, rather than being hard-coded or requiring explicit code implementations for each transition.",TRUE
2311.07076,2023-11-13,{On the Discussion of Large Language Models:,prompt,auto adjust according to specific metric,"The paper discusses both prompt engineering (Chain-of-Thought, In-Context Learning, emotional prompt) and multi-agent discussion mechanisms (Multi-Agent Debate, MAD, ReConcile, Conquer-and-Merge Discussion). It systematically analyzes the interplay between prompt decorators and discussion mechanisms, justifying multi-agent discussions from symmetry perspectives.",,"The paper examines both fixed pipelines (Chain-of-Thoughts Self-Consistency) and adaptive pipelines (Tree-of-Thoughts, Graph-of-Thoughts, Cumulative Reasoning). It also discusses emergent pipelines where reasoning processes self-organize through multi-agent interactions without predefined structures, and proposes a scalable mechanism (CMD) with simple prompts.",TRUE
2410.02953,2024-10-03,Unlocking Structured Thinking in Language Models with Cognitive Prompting,prompt,hard code style_auto adjust leave to LLM,"The paper focuses on cognitive prompting methods that guide LLMs through structured reasoning operations. This involves designing specific prompt structures and sequences to enhance problem-solving capabilities, which falls under prompt engineering.",,"The paper introduces three variants of cognitive prompting: deterministic sequence (hard-coded structure), self-adaptive variant (LLM dynamically selects operations), and hybrid variant (combines generated solutions as few-shot examples). This spans from hard-coded approaches to auto-adjustment by LLM.",TRUE
2311.06318,2023-11-10,Knowledge-Augmented-Large-Language-Models-for-Personalized-Contextual-Query-Suggestion,external data mgr,hard code style_code impls,"The paper focuses on augmenting LLM prompts with user-specific context from search histories and knowledge graphs, which falls under external data management (RAG-like approach) rather than training, fine-tuning, or multi-agent systems.",,"The system constructs and leverages a pre-defined, structured knowledge store based on user interactions and public knowledge graphs, indicating a hard-coded or code-implemented approach rather than dynamic LLM-driven adjustments.",TRUE
2012.15723,2020-12-31,Making Pre-trained Language Models Better Few-shot Learners,prompt,code impls_auto adjust according to specific metric,"This paper focuses on improving few-shot learning capabilities of pre-trained language models, which involves adapting existing models to new tasks with minimal examples. This falls under the prompt category as it deals with how to effectively use and adapt pre-trained models through prompting strategies rather than retraining the model architecture or implementing external data management systems.",,"The paper proposes methods to enhance few-shot learning, which typically involves systematic approaches to prompt construction and optimization. This would involve code implementations and potentially auto-adjustment strategies to optimize prompt effectiveness, though the specific implementation details would determine the exact approach.",TRUE
2203.11171,2022-03-21,Self-Consistency Improves Chain of Thought Reasoning in Language Models,prompt,auto adjust according to specific metric,"The paper focuses on a decoding strategy called self-consistency that works with chain-of-thought prompting. This involves sampling multiple reasoning paths from a pre-trained language model and selecting the most consistent answer, which falls under prompt engineering techniques rather than model training or external data management.",,The self-consistency method involves automatically sampling multiple reasoning paths from the language model and then automatically selecting the most consistent answer through marginalization. This represents an auto-adjust approach based on consistency metrics rather than hard-coded rules or manual implementation.,TRUE
2005.11401,2020-05-22,Retrieval-Augmented Generation (Title needed),external data mgr,code impls,"The paper describes Retrieval-Augmented Generation (RAG), which combines a pre-trained retriever (query encoder + document index) with a pre-trained seq2seq generator. This architecture uses external document retrieval to augment generation, falling under the 'external data mgr' category as it involves retrieving and using external documents during the generation process.",,"The approach involves fine-tuning the retriever and generator end-to-end, indicating that the retrieval and generation components are optimized together through training. This suggests a 'code impls' approach where the system is implemented and trained with specific parameters, rather than being hard-coded or left entirely to LLM auto-adjustment.",TRUE
